{
	"name": "train-and-forecast-cash-weekly-sched",
	"properties": {
		"folder": {
			"name": "Cash Flow Model"
		},
		"nbformat": 0,
		"nbformat_minor": 0,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			}
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "aafbf5fe-19b1-4cbe-9d56-1c37dda3ddfb"
					}
				},
				"source": [
					"# Import the required Libraries\n",
					"import logging\n",
					"import pandas as pd\n",
					"import numpy as np\n",
					"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
					"from datetime import datetime\n",
					"import calendar as cl\n",
					"import seaborn\n",
					"import pickle\n",
					"from scipy.stats import norm\n",
					"from scipy import signal\n",
					"from scipy import stats\n",
					"from scipy.special import inv_boxcox\n",
					"from sklearn.decomposition import PCA\n",
					"import fbprophet\n",
					"from fbprophet import Prophet\n",
					"from fbprophet.make_holidays import make_holidays_df\n",
					"from datetime import timedelta\n",
					"from sklearn.preprocessing import StandardScaler\n",
					"from sklearn.linear_model import HuberRegressor, LinearRegression\n",
					"from pandas.tseries.offsets import MonthBegin, MonthEnd\n",
					"from pandas.tseries.offsets import DateOffset\n",
					"import statsmodels\n",
					"import statsmodels.api as sm\n",
					"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
					"import matplotlib as mpl\n",
					"import matplotlib.pyplot as plt\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark import SparkContext\n",
					"from pyspark.sql import SQLContext\n",
					"from pyspark.sql.session import SparkSession\n",
					"from pyspark.sql.functions import  pandas_udf, PandasUDFType, unix_timestamp\n",
					"from pyspark.sql.types import DoubleType, StructField, StringType, StructType\n",
					"from pyspark.sql import types \n",
					"from pyspark.sql.types import *\n",
					"from pyspark.ml.linalg import Vectors\n",
					"from pyspark.ml.feature import VectorAssembler\n",
					"from pyspark.ml.classification import DecisionTreeClassifier\n",
					"from functools import reduce\n",
					"import databricks.koalas as ks\n",
					"import networkx as nx\n",
					"import dill as pickle\n",
					"import pymc3 as pm \n",
					"from sklearn.preprocessing import LabelEncoder\n",
					"from sklearn.preprocessing import KBinsDiscretizer\n",
					"import random\n",
					"from collections import defaultdict"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "f3bc2e73-9eef-44db-ae84-1134043df3a8"
					}
				},
				"source": [
					"# Set Logging level to Error\n",
					"logger = spark._jvm.org.apache.log4j\n",
					"logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
					"logging.basicConfig()\n",
					"logging.getLogger().setLevel(logging.ERROR)"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "1162f57c-9729-49b0-ab03-a3017d061761"
					}
				},
				"source": [
					"# SQL statements to load the charge and non-charge data.\n",
					"sql_stmt_cash = \"\"\"SELECT DISCHARGE_DATE as \"Disch Date\"\n",
					",PATIENT_TYPE_NAME\n",
					"  ,Region_Name\n",
					"    , CASE\n",
					"        WHEN Tx_Fin_Class_SubCategory_Name = 'Undefined' Then 'Commercial' \n",
					"        ELSE Tx_Fin_Class_SubCategory_Name End As \"Transaction Financial Class Name\"\n",
					"    , CASE\n",
					"        WHEN Tx_Major_Payor_Name = 'Undefined' Then 'NO MAJOR PAYOR'\n",
					"        ELSE Tx_Major_Payor_Name End As \"Transaction Major Payor\"\n",
					"  ,   TX_POST_DATE As \"Transaction Post Date\"\n",
					"    , Sum(Cash_Amount_Net_Refunds) As \"Payment Net Refunds\"\n",
					"\n",
					"  FROM \"HI_DB\".\"DMS_HOSPITAL_BILLING\".\"ENTERPRISE_HB_NONCHARGE_TRANSACTION\"\n",
					"\n",
					"--WHERE TX_Post_Date >= '2020-1-1' \n",
					"-- AND TX_Post_Date <= DATEADD(Day ,-1, current_date)\n",
					"WHERE Cash_Amount_Net_Refunds <> 0\n",
					"\n",
					"--  and DISCHARGE_DATE >= '2020-5-1'\n",
					"  \n",
					"GROUP BY \n",
					"   PATIENT_TYPE_NAME\n",
					"   , Region_Name\n",
					",      DISCHARGE_DATE\n",
					"    , CASE\n",
					"        WHEN Tx_Fin_Class_SubCategory_Name = 'Undefined' Then 'Commercial' \n",
					"        ELSE Tx_Fin_Class_SubCategory_Name End\n",
					"    , CASE\n",
					"        WHEN Tx_Major_Payor_Name = 'Undefined' Then 'NO MAJOR PAYOR'\n",
					"        ELSE Tx_Major_Payor_Name End \n",
					",     TX_Post_Date\n",
					"\n",
					"\n",
					";\n",
					"\"\"\"\n",
					"\n",
					"\n",
					"sql_stmt_charge = \"\"\"\n",
					"SELECT Account_Class_Name As \"Account Class\"\n",
					"      , Patient_Type_Name As \"Patient Type\"\n",
					"     , Region_Name As \"Region Name\"\n",
					"     , Facility_Name As \"Facility Name\"\n",
					"    , CASE\n",
					"       WHEN Acct_Fin_Class_SubCategory_Name = 'Undefined' Then 'Commercial' \n",
					"       ELSE Acct_Fin_Class_SubCategory_Name End As \"Account Financial Class Name\"\n",
					"--    , Acct_Payor_Name As \"Account Payor Name\"\n",
					"    , CASE\n",
					"        WHEN Acct_Major_Payor_Name = 'Undefined' Then 'NO MAJOR PAYOR'\n",
					"        ELSE Acct_Major_Payor_Name End As \"Account Major Payor\"\n",
					"--  ,   TX_Post_Date_Month_Short_Name As \"Transaction Post Date Month\"\n",
					",    TX_Post_Date As \"Transaction Post Date\"\n",
					"    , Sum(Charge_Amount) As \"Charge Amount\"\n",
					"-- From \"HI_DB_DEV\".\"DMS_HOSPITAL_BILLING\".\"ENTERPRISE_HB_CHARGE_TRANSACTION\"\n",
					"FROM \"HI_DB\".\"DMS_HOSPITAL_BILLING\".\"ENTERPRISE_HB_CHARGE_TRANSACTION\"\n",
					"WHERE TX_Post_Date >= '2014-12-01'\n",
					"--AND TX_Post_Date <= '2019-12-31'\n",
					"AND TX_Post_Date <= DATEADD(Day ,-1, current_date)\n",
					"AND Charge_Amount <> 0\n",
					"GROUP BY\n",
					"    Account_Class_Name \n",
					"    , Patient_Type_Name\n",
					"    , Region_Name\n",
					"    , Facility_Name\n",
					"  ,   CASE\n",
					"        WHEN Acct_Fin_Class_SubCategory_Name = 'Undefined' Then 'Commercial' \n",
					"        ELSE Acct_Fin_Class_SubCategory_Name End\n",
					"--    , Acct_Payor_Name\n",
					"    , CASE\n",
					"        WHEN Acct_Major_Payor_Name = 'Undefined' Then 'NO MAJOR PAYOR'\n",
					"        ELSE Acct_Major_Payor_Name End\n",
					"--    , TX_Post_Date_Month_Short_Name\n",
					",     TX_Post_Date\n",
					";\n",
					"\"\"\"\n",
					"\n",
					"\n",
					"\"\"\"\n",
					"Set the options to pull the training data on charge and cash transactions from CDW.\n",
					"\"\"\"\n",
					"user = dbutils.secrets.get('KV-HI-Data-science','DataScienceSAUser')\n",
					"password = dbutils.secrets.get('KV-HI-Data-science','DataScienceSAPwd')\n",
					"options = dict(sfUrl=\"psjh_prod.west-us-2.azure.snowflakecomputing.com\",\n",
					"               sfUser=user,\n",
					"               sfPassword=password,\n",
					"               sfDatabase=\"HI_DB\",\n",
					"               sfSchema=\"DMS_HOSPITAL_BILLING\",\n",
					"               sfWarehouse=\"HI_DATASCIENCE_WH\")"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "52fe6db8-c845-4a67-bdf3-37d76597e33a"
					}
				},
				"source": [
					"# Constants\n",
					"time_stamp_col = 'Transaction Post Date'\n",
					"response_var = 'Payment Net Refunds'\n",
					"count_col = 'Counts'\n",
					"dep_var = 'Payment Net Refunds'\n",
					"discharge_date = 'Disch Date'\n",
					"txn_date_format = '%Y-%m-%d %H:%M:%S'\n",
					"resample_freq = 'D'\n",
					"graph_model_path = \"/dbfs/FileStore/udai/cashflowgraph.pkl\"\n",
					"\n",
					"# Non-Charge (Cash) Columns lists\n",
					"df_non_charge_columns = [discharge_date, 'PATIENT_TYPE_NAME', 'REGION_NAME', 'Transaction Financial Class Name', 'Transaction Major Payor', time_stamp_col, response_var]\n",
					"dummy_vars_non_charge = ['PATIENT_TYPE_NAME', 'REGION_NAME', 'Transaction Financial Class Name', 'Transaction Major Payor']\n",
					"\n",
					"# Charge Data columns\n",
					"charge_col_names = ['Account Class', 'Patient Type', \"Region Name\", 'Facility Name' , \"Account Financial Class Name\", \"Account Major Payor\", \"Transaction Post Date\", \"Charge Amount\"]\n",
					"dummy_vars_charge_data = ['Account Class', 'Patient Type', 'Region Name', 'Facility Name', 'Account Financial Class Name','Account Major Payor']\n",
					"\n",
					"# Parameters for training and forecasting the cash\n",
					"lags = list(range(1, 45))\n",
					"dof = 5\n",
					"train_from_date = '2015-01-01'\n",
					"past_fcast_start_ = '2018-01-01'\n",
					"future_fcast_till_ = '2022-03-01'\n",
					"train_graph_from_date = '2020-01-01'\n",
					"charge_amount = 'Charge Amount'\n",
					"arima_order, sarima_order = (0,0,0), (2,0,2,7)\n",
					"year_list = [2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022]\n",
					"country_name = 'US'\n",
					"interval_width = 0.95\n",
					"# future_fcast_start_date = (datetime.today() - MonthEnd(n=1)).strftime(\"%Y-%m-%d\")\n",
					"future_fcast_start_date = (datetime.today()).strftime(\"%Y-%m-%d\")\n",
					"\n",
					"# Final Forecast dataframes columns.\n",
					"cash_lower = \"Predicted_Net_Refund_Lower\"\n",
					"cash_upper = \"Predicted_Net_Refund_Upper\"\n",
					"cash_ = \"Predicted_Net_Refund\"\n",
					"cash_actual = \"Payment_Net_Refunds\"\n",
					"future_fcast_columns = [cash_, cash_lower, cash_upper, 'DATE']\n",
					"\n",
					"fcast_col_name_dict = {'PATIENT_TYPE_NAME':\"PATIENT_TYPE_NAME\", 'REGION_NAME':\"REGION_NAME\",'Transaction Financial Class Name':\"Transaction Financial Class Name\", 'Transaction Major Payor':\"Transaction Major Payor\",'Payment Net Refunds':\"Payment Net Refunds\",\"DATE\":\"DATE\",cash_:'Predicted_Net_Refunds',cash_lower:'Predicted_Net_Refunds_Lower',cash_upper:'Predicted_Net_Refunds_Upper'}\n",
					"\n",
					"fcast_col_name_dict_1 = {'PATIENT_TYPE_NAME':\"PATIENT_TYPE_NAME\", 'REGION_NAME':\"REGION_NAME\",'Transaction Financial Class Name':\"Transaction_Financial_Class_Name\", 'Transaction Major Payor':\"Transaction_Major_Payor\",'Payment Net Refunds':cash_actual,\"DATE\":\"DATE\",'Predicted_Net_Refunds':'Predicted_Net_Refunds','Predicted_Net_Refunds_Lower':'Predicted_Net_Refunds_Lower','Predicted_Net_Refunds_Upper':'Predicted_Net_Refunds_Upper'}\n",
					"\n",
					"# Constants to be used for pushing forecast to CDW\n",
					"user = dbutils.secrets.get('KV-HI-Data-science', 'DataScienceSAUser')\n",
					"password = dbutils.secrets.get('KV-HI-Data-science', 'DataScienceSAPwd')\n",
					"sfUrl = \"psjh_prod.west-us-2.azure.snowflakecomputing.com\"\n",
					"sfDatabase=\"REVENUE_CYCLE\",\n",
					"sfSchema=\"DEV_RCS\",\n",
					"sfWarehouse=\"REVCYCLE_WH\"\n",
					"table_name = \"CASH_FORECAST_SPLINE_TEST\""
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "2ac287dd-57a2-4406-a622-9a0637c23e17"
					}
				},
				"source": [
					"\"\"\"\n",
					"Methods used for cash forecasting.\n",
					"\"\"\" \n",
					"\n",
					"def clean_data(df, col_names, time_stamp_col):\n",
					"  \"\"\"\n",
					"  Set index of the charge data. Drop NAs.\n",
					"  \"\"\"\n",
					"  df.index = df[time_stamp_col]\n",
					"  df = df.drop(columns=[time_stamp_col])\n",
					"  df = df.dropna()\n",
					"  return df\n",
					"\n",
					"def aggregate_charge(pd_df_, dummy_vars):\n",
					"  \"\"\"\n",
					"  Drop columns that are not used for modeling and aggregate by day.\n",
					"  \"\"\"\n",
					"  pd_df_ = pd_df_.drop(columns=dummy_vars)\n",
					"  df_resampled = pd_df_.resample(resample_freq, label='left',closed='left').sum().reset_index()\n",
					"  return df_resampled\n",
					"\n",
					"def pre_process_charge(df_c):\n",
					"  df_c = df_c.toPandas()\n",
					"  df_c.columns = charge_col_names\n",
					"  df_c = clean_data(df_c, charge_col_names, time_stamp_col)\n",
					"  df_c = df_c.astype({charge_amount: float})\n",
					"  return df_c\n",
					"\n",
					"def pre_process_cash(df_non_charge):\n",
					"  # Set the schema of the columns to be passed to the pandas UDF.\n",
					"  schema = StructType([StructField(time_stamp_col, TimestampType()),StructField(dep_var, DoubleType())])\n",
					"  df_non_charge = reduce(lambda df_non_charge, idx: df_non_charge.withColumnRenamed(df_non_charge.schema.names[idx], df_non_charge_columns[idx]), range(len(df_non_charge.schema.names)), df_non_charge)\n",
					"  df_non_charge_pd = df_non_charge.toPandas()\n",
					"  return df_non_charge_pd\n",
					"\n",
					"def split_data(df_, train_ratio):\n",
					"  split_idx = int(df_.shape[0]*train_ratio)\n",
					"  train_df = df_.iloc[:split_idx]\n",
					"  test_df = df_.iloc[split_idx:df_.shape[0]-1]\n",
					"  return train_df, test_df\n",
					"\n",
					"def get_train_and_test(pd_df, train_ratio):\n",
					"  train_df, test_df = split_data(pd_df, train_ratio)\n",
					"  train_df = train_df.set_index(time_stamp_col)\n",
					"  test_df = test_df.set_index(time_stamp_col)\n",
					"  return train_df, test_df\n",
					"\n",
					"def get_endog_and_exog(train_df):\n",
					"  endog = train_df[dep_var]\n",
					"  exog = train_df.drop(columns=[dep_var])\n",
					"  return endog, exog\n",
					"\n",
					"# Add holiday dummies to the data.\n",
					"def add_holidays(df):\n",
					"  \"\"\"\n",
					"  Add dummies for holidays, holidays for monday and friday.\n",
					"  \"\"\"\n",
					"  cal = calendar()\n",
					"  holidays = cal.holidays(start=df.index.min(), end=df.index.max())\n",
					"  df = df.assign(Holiday=0)\n",
					"  df = df.assign(holiday_monday=0)\n",
					"  df = df.assign(holiday_friday=0)\n",
					"#   df = df.assign(long_weekend=0)\n",
					"  df.loc[df.index.isin(holidays), 'Holiday'] = 1\n",
					"  df.loc[(df.index.isin(holidays)) & (df.index.dayofweek == 0), 'holiday_monday'] = 1\n",
					"  df.loc[(df.index.isin(holidays)) & (df.index.dayofweek == 4), 'holiday_friday'] = 1\n",
					"#   df.loc[(df.index.isin(holidays)) & ((df.index.dayofweek == 4) | (df.index.dayofweek == 0)), 'long_weekend'] = 1\n",
					"  return df\n",
					"\n",
					"# Add date based dummies to the data.\n",
					"def add_date_dummies(df):\n",
					"  df = df.assign(saturday=0)\n",
					"  df = df.assign(sunday=0)\n",
					"  df.loc[(df.index.dayofweek == 5), 'saturday'] = 1\n",
					"  df.loc[(df.index.dayofweek == 6), 'sunday'] = 1\n",
					"  return df\n",
					"\n",
					"def add_weekday_dummies(df):\n",
					"  \"\"\"\n",
					"  Add dummies for day of week.\n",
					"  \"\"\"\n",
					"  names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
					"  for i, x in enumerate(names):\n",
					"      df[x] = (df.index.get_level_values(0).weekday == i).astype(int)\n",
					"  df = df.drop(columns=['Monday', 'Saturday', 'Sunday'])\n",
					"  return df\n",
					"\n",
					"def add_month_dummies(df):\n",
					"  \"\"\"\n",
					"  Add dummies for month.\n",
					"  \"\"\"\n",
					"  names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
					"  for i, x in enumerate(names):\n",
					"      df[x] = (df.index.get_level_values(0).month == i).astype(int)\n",
					"  df = df.drop(columns=['Aug'])\n",
					"  return df\n",
					"\n",
					"def get_busdays_remaining(df):\n",
					"  \"\"\"\n",
					"  Get the number of business days remaining in the month.\n",
					"  \"\"\"\n",
					"  df['Transcation Post Date'] = df.index\n",
					"  df['Transcation Post Date Shifted'] = df['Transcation Post Date']  + pd.offsets.MonthBegin(n=1)\n",
					"  df['remaining_busdays_in_month'] = np.busday_count(df['Transcation Post Date'].values.astype('datetime64[D]'),df['Transcation Post Date Shifted'].values.astype('datetime64[D]'))\n",
					"  df = df.drop(columns=['Transcation Post Date Shifted', 'Transcation Post Date'])\n",
					"  return df\n",
					"\n",
					"def add_fourier_terms(df):\n",
					"  \"\"\"\n",
					"  Add the Fourier Terms to model multiple seasonalities in the cash data.\n",
					"  \"\"\"\n",
					"  df['sin365'] = np.sin(2 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['cos365'] = np.cos(2 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['sin_365_2'] = np.sin(5 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['cos_365_2'] = np.cos(5 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['sin_365_4'] = np.sin(4 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['cos_365_4'] = np.cos(4 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['sin_365_7'] = np.sin(7 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['cos_365_7'] = np.cos(7 * np.pi * df.index.dayofyear/365.25)\n",
					"  df['sin7'] = np.sin(2 * np.pi * df.index.dayofyear/7)\n",
					"  df['cos7'] = np.cos(2 * np.pi * df.index.dayofyear/7)\n",
					"  df['sin_7_2'] = np.sin(5 * np.pi * df.index.dayofyear/7)\n",
					"  df['cos_7_2'] = np.cos(5 * np.pi * df.index.dayofyear/7)\n",
					"  df['sin_7_7'] = np.sin(7 * np.pi * df.index.dayofyear/7)\n",
					"  df['cos_7_7'] = np.cos(7 * np.pi * df.index.dayofyear/7)\n",
					"  df['sin_7_4'] = np.sin(4 * np.pi * df.index.dayofyear/7)\n",
					"  df['cos_7_4'] = np.cos(4 * np.pi * df.index.dayofyear/7)\n",
					"  df['sin30'] = np.sin(2 * np.pi * df.index.dayofyear/30.25)\n",
					"  df['cos30'] = np.cos(2 * np.pi * df.index.dayofyear/30.25)\n",
					"  df['sin_30_2'] = np.sin(5 * np.pi * df.index.dayofyear/30.25)\n",
					"  df['cos_30_2'] = np.cos(5 * np.pi * df.index.dayofyear/30.25)\n",
					"  return df\n",
					"\n",
					"# Box-Cox and inverse box cox Transformation of the variables.\n",
					"def box_cox_transform(endog):\n",
					"  \"\"\"\n",
					"  Perform Box-Cox Transformation and set index.\n",
					"  \"\"\"\n",
					"  endog_t, _ = stats.boxcox(endog)\n",
					"  print(f\"The lambda parameter: {_}\")\n",
					"  endog_t = pd.Series(np.array(endog_t))\n",
					"  endog_t.index = endog.index\n",
					"  return endog_t, _\n",
					"\n",
					"def invert_bx_cx(arr, lam):\n",
					"  \"\"\"\n",
					"  Invert the box-cox transformed values given the array and the lambda parameter.\n",
					"  \"\"\"\n",
					"  y_actual = inv_boxcox(arr, lam)\n",
					"  y_actual[np.isnan(y_actual)] = 0\n",
					"  return y_actual\n",
					"\n",
					"# Get the lagged dataframe using the charge data.\n",
					"def get_lagged_df(df, col_name, lags):\n",
					"  \"\"\"\n",
					"  Create and append lagged data for the given column.\n",
					"  \"\"\"\n",
					"  transformed_arr = pd.Series(df[col_name])\n",
					"  transformed_arr.index = df.index\n",
					"  transformed_arr = pd.DataFrame(transformed_arr)\n",
					"  transformed_arr.columns = [col_name]\n",
					"  # Month lagged variables for the charge data.\n",
					"  lagged_df = transformed_arr.assign(**{f'{col_name} (t-{lag})': transformed_arr[col_name].shift(lag) for lag in lags})\n",
					"  lagged_df = lagged_df.dropna()\n",
					"  return lagged_df\n",
					"\n",
					"\n",
					"# # Box-Cox Transform and embed in dataframe.\n",
					"def transform_and_embed(df, col_name, is_training, lmbda):\n",
					"  \"\"\"\n",
					"  Create and append lagged data for the given column.\n",
					"  \"\"\"\n",
					"  _arr = df[col_name]\n",
					"  if is_training is True:\n",
					"    _arr, lmbda = box_cox_transform(_arr)\n",
					"  else:\n",
					"    _arr = box_cox_transform(_arr, lmbda)\n",
					"  _arr = pd.DataFrame(_arr)\n",
					"  _arr.columns = [col_name]\n",
					"  df_transformed = df.drop(columns=[col_name]).join(_arr)\n",
					"  return df_transformed, lmbda\n",
					"\n",
					"def endog_exog_for_statsmodels(df, bx_cx=None):\n",
					"  \"\"\"\n",
					"  Create the endogenous and exogenous data for fitting statsmodels models.\n",
					"  \"\"\"\n",
					"  endog, exog = get_endog_and_exog(df)\n",
					"  endog = endog.astype(float)\n",
					"  if bx_cx is True:\n",
					"    endog_t, lam = box_cox_transform(endog)\n",
					"    return endog_t, exog, lam\n",
					"  else:\n",
					"    return endog, exog, None\n",
					"  \n",
					"# Add the week of the month as categorical variables\n",
					"cl.setfirstweekday(0)\n",
					"\n",
					"def get_week_of_month(row):\n",
					"    year, month, day = int(row['year']), int(row['month']), int(row['Day'])\n",
					"    x = np.array(cl.monthcalendar(year, month))\n",
					"    week_of_month = np.where(x==day)[0][0] + 1\n",
					"    return(week_of_month)\n",
					"\n",
					"def get_week_dummies(df):\n",
					"  \"\"\"\n",
					"  Get the week of the month.\n",
					"  \"\"\"\n",
					"  df['Day'] = df.index.day\n",
					"  df['month'] = df.index.month\n",
					"  df['year'] = df.index.year\n",
					"  df['week'] = df.apply(lambda row: get_week_of_month(row), axis=1)\n",
					"  df = df.drop(columns=['Day', 'month', 'year'])\n",
					"  return df\n",
					"\n",
					"def add_weekend_dummies(df):\n",
					"  \"\"\"\n",
					"  Add dummies for which weekend of the month is it.\n",
					"  \"\"\"\n",
					"  wknd_dummy = get_week_dummies(df)\n",
					"  wknd_dummy = pd.get_dummies(wknd_dummy, columns=['week'], drop_first=False, prefix='weekend_saturday')\n",
					"  for week in ['weekend_saturday_1' ,'weekend_saturday_2', 'weekend_saturday_3', 'weekend_saturday_4', 'weekend_saturday_5',  'weekend_saturday_6']:\n",
					"    wknd_dummy.loc[((wknd_dummy[week] == 1) & (wknd_dummy['saturday'] != 1)), week] = 0\n",
					"  wknd_dummy = wknd_dummy.drop(columns=['weekend_saturday_6'])\n",
					"  wknd_dummy = get_week_dummies(wknd_dummy)\n",
					"  wknd_dummy = pd.get_dummies(wknd_dummy, columns=['week'], drop_first=False, prefix='weekend_sunday')\n",
					"  for week in ['weekend_sunday_1' ,'weekend_sunday_2', 'weekend_sunday_3', 'weekend_sunday_4', 'weekend_sunday_5', 'weekend_sunday_6']:\n",
					"    wknd_dummy.loc[(wknd_dummy[week] == 1) & (wknd_dummy['sunday'] != 1) , week] = 0\n",
					"  wknd_dummy = wknd_dummy.drop(columns=['weekend_sunday_6'])\n",
					"  wknd_dummy = get_week_dummies(wknd_dummy)\n",
					"  final_df = pd.get_dummies(wknd_dummy, columns=['week'], drop_first=True)\n",
					"  return final_df\n",
					"\n",
					"def replace_by_mean(df, col, is_training, avg):\n",
					"  \"\"\"\n",
					"  Replace the mean for the negative values.\n",
					"  \"\"\"\n",
					"  if is_training is True:\n",
					"    avg = np.mean(df[col])\n",
					"    df[df[col] < 0] = avg\n",
					"  else:\n",
					"    df[df[col] < 0] = avg\n",
					"  return df, avg\n",
					"\n",
					"def add_features(df):\n",
					"  # # Add relevant variables for regression.\n",
					"  df = add_date_dummies(df)\n",
					"  df = add_holidays(df)\n",
					"  df = add_month_dummies(df)\n",
					"  df = add_weekday_dummies(df)\n",
					"  df = get_busdays_remaining(df)\n",
					"  df = add_weekend_dummies(df)\n",
					"  df = add_fourier_terms(df)\n",
					"  return df\n",
					"  \n",
					"def prepare_charge_data(df_c, dummy_vars_charge_data, lags):\n",
					"  \"\"\"\n",
					"  Prepare the charge data: Clean, Add lags of charge for fitting model.\n",
					"  \"\"\"\n",
					"  charge_df_dummies = aggregate_charge(df_c, dummy_vars=dummy_vars_charge_data)\n",
					"  charge_df_dummies.index = charge_df_dummies[time_stamp_col]\n",
					"  charge_df_dummies = charge_df_dummies.drop(columns=[time_stamp_col])\n",
					"  charge_df = charge_df_dummies.dropna()\n",
					"  charge_df = add_features(charge_df)\n",
					"  charge_df, avg = replace_by_mean(charge_df, charge_amount, is_training=True, avg=None)\n",
					"  # Get Lagged charge Amounts.\n",
					"  charge_lagged_df = get_lagged_df(charge_df, charge_amount, lags)\n",
					"  final_charge_df = charge_lagged_df.join(charge_df.drop(columns=[charge_amount]))\n",
					"  final_charge_df = final_charge_df.dropna()\n",
					"  return final_charge_df\n",
					"\n",
					"def append_dep_var(df_non_charge_pd, final_charge_df):\n",
					"  \"\"\"\n",
					"  Append the dependent variable column to the data matrix to be used for fitting the model.\n",
					"  \"\"\"\n",
					"  if time_stamp_col in list(df_non_charge_pd.columns):\n",
					"    df_non_charge_pd.index = df_non_charge_pd[time_stamp_col]\n",
					"    df_non_charge_pd = df_non_charge_pd.drop(columns=[time_stamp_col])\n",
					"  if discharge_date in list(df_non_charge_pd.columns):\n",
					"    df_non_charge_pd = df_non_charge_pd.drop(columns=[discharge_date])\n",
					"\n",
					"  df_non_charge_pd_dummies = aggregate_charge(df_non_charge_pd, dummy_vars=dummy_vars_non_charge)\n",
					"  df_non_charge_pd_dummies.index = df_non_charge_pd_dummies[time_stamp_col]\n",
					"  df_non_charge_pd_dummies = df_non_charge_pd_dummies[df_non_charge_pd_dummies.index.year >= 2015]\n",
					"  dep_var_arr = df_non_charge_pd_dummies[response_var]\n",
					"\n",
					"  # Get the Final DataFrame for model training\n",
					"  if response_var not in list(final_charge_df.columns):\n",
					"    final_df = final_charge_df.join(dep_var_arr)\n",
					"    final_df = final_df.dropna()\n",
					"  return final_df\n",
					"\n",
					"def add_spline_basis(df, dof, cubic, col_name, degree, is_test, spline_=None):\n",
					"  \"\"\"\n",
					"  Cubic and BSplines of the explanatory variables.\n",
					"  \"\"\"\n",
					"  df_copy = df.copy(deep=True)\n",
					"  col_list = list(df.columns)\n",
					"  for i in range(len(col_list)):\n",
					"    col = col_list[i]\n",
					"    if col_name in col:\n",
					"      x = df[col]\n",
					"      if cubic is True:\n",
					"        col_names = ['spline_' + col + '_' + str(i) for i in range(1, dof+1)]\n",
					"      else:\n",
					"        col_names = ['spline_' + col + '_' + str(i) for i in range(1, dof)]\n",
					"        \n",
					"      if is_test is False:\n",
					"        if cubic is True:\n",
					"          spline = statsmodels.gam.smooth_basis.CyclicCubicSplines(x, df=[dof])\n",
					"        else:\n",
					"          if degree is None:\n",
					"            spline = statsmodels.gam.smooth_basis.BSplines(x, df=[dof], degree=[2], knot_kwds=[{'spacing':'quantile'}])\n",
					"          else:\n",
					"            spline = statsmodels.gam.smooth_basis.BSplines(x, df=[dof], degree=[degree], knot_kwds=[{'spacing':'quantile'}])\n",
					"        transformed = spline.transform(np.array(x))\n",
					"      else:\n",
					"        transformed = spline_.transform(np.array(x))\n",
					"      transformed_df = pd.DataFrame(transformed)\n",
					"      transformed_df.index = df.index\n",
					"      transformed_df.columns = col_names\n",
					"      df_copy = df_copy.join(transformed_df)\n",
					"      df_copy = df_copy.drop(columns=[col])\n",
					"  if is_test is False:\n",
					"    return df_copy, spline\n",
					"  else:\n",
					"    return df_copy\n",
					"  \n",
					"def fit_and_predict(ts, train_till_date, forecast_till_date, in_sample):\n",
					"  \"\"\"\n",
					"  Fit a SARIMA model to predict charge amount.\n",
					"  \"\"\"\n",
					"  order, seasonal_order = (0,0,1), (2,0,2,7)\n",
					"  steps = forecast_till_date - train_till_date\n",
					"  print(f\"Steps: {steps}\")\n",
					"  ts_train = ts[:train_till_date]\n",
					"  charge_model = sm.tsa.statespace.SARIMAX(endog=ts_train, order=order, seasonal_order=seasonal_order)\n",
					"  charge_model = charge_model.fit(disp=False)\n",
					"  if in_sample is True:\n",
					"    fcast = charge_model.get_prediction()\n",
					"    fcast = fcast.predicted_mean\n",
					"  else:\n",
					"    fcast = charge_model.forecast(steps=steps.days)\n",
					"  fcast.index.name = time_stamp_col\n",
					"  fcast.name = charge_amount\n",
					"  return fcast\n",
					"\n",
					"def get_test_data(test_exog, lags, date_, test_till_date, charge_fcast):\n",
					"  \"\"\"\n",
					"  Get the forecasted exogenous matrix - data used to predict cash using predicted charge.\n",
					"  \"\"\"\n",
					"  days = lags[-1]\n",
					"  date_including_lags = date_ + DateOffset(days=-days)\n",
					"  test_exog = test_exog.loc[date_including_lags: test_till_date]\n",
					"  test_exog_ = test_exog.drop(columns=[cols for cols in list(test_exog.columns) if \"Charge Amount (\" in cols])\n",
					"  charge_series = test_exog_[charge_amount]\n",
					"  new_charge = charge_fcast.to_frame().merge(charge_series.to_frame(), how='right', right_index=True, left_index=True)\n",
					"  new_charge.loc[new_charge['Charge Amount_x'].isnull(),'Charge Amount_x'] = new_charge['Charge Amount_y']\n",
					"  new_charge = new_charge.drop(columns='Charge Amount_y')\n",
					"  test_exog_[charge_amount] = new_charge['Charge Amount_x']\n",
					"  test_exog_lagged = get_lagged_df(test_exog_, charge_amount, lags)\n",
					"  test_exog_df = test_exog_lagged.merge(test_exog_)\n",
					"  test_exog_df.index = test_exog_lagged.index\n",
					"  return test_exog_df\n",
					"  \n",
					"# Fit Penalized regression models.\n",
					"def fit_regularized_ols(train_exog, train_endog, fcstd_test_exog, scaled, alpha):\n",
					"  \"\"\"\n",
					"  Fit a regularized OLS using statsmodels.\n",
					"  \"\"\"\n",
					"  if scaled is True:\n",
					"    scaler = StandardScaler()\n",
					"    scaler.fit(train_exog)\n",
					"    train_exog = scaler.transform(train_exog)\n",
					"    fcstd_test_exog = scaler.transform(fcstd_test_exog)\n",
					"  model = sm.OLS(train_endog, train_exog)\n",
					"  results = model.fit_regularized(method='elastic_net', alpha=alpha , L1_wt=0)\n",
					"  preds = results.predict(fcstd_test_exog)\n",
					"  return preds, model\n",
					"\n",
					"def fit_ridge_ols(train_exog, train_endog, fcstd_test_exog, scaled, alpha):\n",
					"  \"\"\"\n",
					"  Fit a regularized OLS using sklearn.\n",
					"  \"\"\"\n",
					"  if scaled is True:\n",
					"    scaler = StandardScaler()\n",
					"    scaler.fit(train_exog)\n",
					"    train_exog = scaler.transform(train_exog)\n",
					"    fcstd_test_exog = scaler.transform(fcstd_test_exog)\n",
					"  from sklearn import linear_model\n",
					"  ridge = linear_model.Ridge(alpha=alpha)\n",
					"  ridge.fit(train_exog, train_endog)\n",
					"  preds = ridge.predict(fcstd_test_exog)\n",
					"  return preds, ridge\n",
					"\n",
					"def forecast_charge(ts, train_till_date, forecast_till_date):\n",
					"  \"\"\"\n",
					"  Forecast the charge amount using SARIMA.\n",
					"  \"\"\"\n",
					"  order, seasonal_order = arima_order, sarima_order\n",
					"  steps = forecast_till_date - train_till_date\n",
					"  ts_train = ts[:train_till_date]\n",
					"  charge_model = sm.tsa.statespace.SARIMAX(endog=ts_train, order=order, seasonal_order=seasonal_order)\n",
					"  charge_model = charge_model.fit(disp=False)\n",
					"  fcast = charge_model.forecast(steps=steps.days)\n",
					"  fcast.index.name = time_stamp_col\n",
					"  fcast.name = charge_amount\n",
					"  return fcast\n",
					"\n",
					"def get_future_forecasts(fcstd_test_exog, model, lmbda):\n",
					"  \"\"\"\n",
					"  Forecast the cash amount and invert to original scale.\n",
					"  \"\"\"\n",
					"  fcasts = model.predict(fcstd_test_exog)\n",
					"  fcasts = invert_bx_cx(fcasts, lmbda)\n",
					"  return fcasts\n",
					"\n",
					"def prep_data_final_fcast(exog, test_till_date, lags):\n",
					"  \"\"\"\n",
					"  Create charge forecast data for model training and add dummies.\n",
					"  \"\"\"\n",
					"  charge_ts = exog[charge_amount]\n",
					"  train_till_date = exog.tail(1).index.item()\n",
					"  charge_fcast = forecast_charge(charge_ts, train_till_date, test_till_date)\n",
					"  print(charge_fcast)\n",
					"  charge_final = charge_ts.append(charge_fcast)\n",
					"  charge_fcast_df = pd.DataFrame(charge_final)\n",
					"  final_exog = get_lagged_df(charge_fcast_df, charge_amount, lags)\n",
					"  final_exog = add_features(final_exog)\n",
					"  return final_exog\n",
					"\n",
					"def append_spline_basis(spline_, b_spline_, final_exog, dof):\n",
					"  \"\"\"\n",
					"  Append Spline Basis for a given degree of freedom for columns: charge amount and business days in month.\n",
					"  \"\"\"\n",
					"  spline_model = spline_[dof]\n",
					"  b_spline_model = b_spline_[dof]\n",
					"  forecast_exog_ = add_spline_basis(final_exog, dof, True, charge_amount, degree=None, is_test=True, spline_=spline_model)\n",
					"  forecast_exog_ = add_spline_basis(forecast_exog_, dof, True, 'remaining_busdays_in_month', degree=None, is_test=True, spline_=b_spline_model)\n",
					"  return forecast_exog_\n",
					" \n",
					"def generate_past_fcasts(date, exog, dof, till_date):\n",
					"  \"\"\"\n",
					"  Split data, add spline basis, fit OLS and generate future forecasts for cash.\n",
					"  \"\"\"\n",
					"  exog = exog.loc[:till_date]\n",
					"  test_index = exog.loc[date:].index\n",
					"  y_true_original_scale = []\n",
					"  preds_original_scale = []\n",
					"  preds_lower_original_scale = []\n",
					"  preds_upper_original_scale = []\n",
					"  charge_ts = final_charge_df[charge_amount]\n",
					"  date_ = pd.to_datetime(date)\n",
					"  while date_ < pd.to_datetime(till_date):\n",
					"    # Create Train/Test Date ranges for slicing the data into train/test.\n",
					"    train_till_date = date_ + DateOffset(days=-1)\n",
					"    test_till_date = date_ + MonthBegin(n=1) + DateOffset(days=-1)\n",
					"    print(f\"Test till Date is: {test_till_date}\")\n",
					"    # Prepare Train and Test Data.\n",
					"    exog = sm.add_constant(exog)\n",
					"    test_exog, test_endog = exog.loc[date_: test_till_date], endog.loc[date_: test_till_date]\n",
					"    train_exog, train_endog = exog.loc[:train_till_date], endog.loc[:train_till_date]\n",
					"    # Get Box-Cox Transformed Payment Net Refunds training set. To be applied on the test set.\n",
					"    mean_refund = np.median(train_endog)\n",
					"    train_endog[train_endog < 0] = mean_refund\n",
					"    train_endog, endog_lambda = box_cox_transform(train_endog)\n",
					"    # Get Test Data using the forecasted charge amount.\n",
					"    print(f\"The dates: {train_till_date}, {test_till_date}\")\n",
					"    charge_fcast = fit_and_predict(charge_ts, train_till_date, test_till_date, False)\n",
					"    fcstd_test_exog = get_test_data(exog, lags, date_, test_till_date, charge_fcast)\n",
					"    # Add Cyclic Cubic Spline Basis.\n",
					"    train_exog, spline_ = add_spline_basis(train_exog, dof, True, charge_amount, degree=None, is_test=False, spline_=None)\n",
					"    fcstd_test_exog = add_spline_basis(fcstd_test_exog, dof, True, charge_amount, degree=None, is_test=True, spline_=spline_)\n",
					"    train_exog, b_spline = add_spline_basis(train_exog, dof, True, 'remaining_busdays_in_month', degree=None, is_test=False, spline_=None)\n",
					"    fcstd_test_exog = add_spline_basis(fcstd_test_exog, dof, True, 'remaining_busdays_in_month', degree=None, is_test=True, spline_=b_spline)\n",
					"    # Fit the model. Generate Predictions.\n",
					"    model = sm.OLS(train_endog, train_exog)\n",
					"    model_fit = model.fit()\n",
					"    preds = model_fit.predict(fcstd_test_exog)\n",
					"    predictions = model_fit.get_prediction(fcstd_test_exog)\n",
					"    preds_summary = predictions.summary_frame(alpha=0.05)\n",
					"    mean_ci_lower = list(invert_bx_cx(preds_summary['mean_ci_lower'], endog_lambda))\n",
					"    mean_ci_upper = list(invert_bx_cx(preds_summary['mean_ci_upper'], endog_lambda))\n",
					"    # Jump to the next month Begining.\n",
					"    date_ =  date_ + MonthBegin(n=1)\n",
					"    # Append forecasts to the main forecast list.\n",
					"    y_true_original_scale = y_true_original_scale + list(test_endog)\n",
					"    preds_original_scale = preds_original_scale + list(invert_bx_cx(preds, endog_lambda))\n",
					"    preds_lower_original_scale = preds_lower_original_scale + mean_ci_lower\n",
					"    preds_upper_original_scale = preds_upper_original_scale + mean_ci_upper\n",
					"  fcast_df = pd.DataFrame({'Payment_Net_Refunds' :y_true_original_scale, cash_: preds_original_scale, cash_lower:preds_lower_original_scale, cash_upper:preds_upper_original_scale}, index=test_index)\n",
					"  return fcast_df, model_fit, spline_, b_spline, endog_lambda\n",
					"  \n",
					"def get_forecasts(df, model, cut_off_date, lmbda):\n",
					"  \"\"\"\n",
					"  Get the lower and upper range for the cash predictions.\n",
					"  \"\"\"\n",
					"  df_ = df.loc[cut_off_date:]\n",
					"  predictions = model.get_prediction(df_)\n",
					"  preds_summary = predictions.summary_frame(alpha=0.05)\n",
					"  preds_summary['mean'] = invert_bx_cx(preds_summary['mean'], lmbda)\n",
					"  preds_summary['mean_ci_lower'] = invert_bx_cx(preds_summary['mean_ci_lower'], lmbda)\n",
					"  preds_summary['mean_ci_upper'] = invert_bx_cx(preds_summary['mean_ci_upper'], lmbda)\n",
					"  return preds_summary\n",
					"\n",
					"### Generate forecasts for six months into the future\n",
					"### Create the dates for forecasting.\n",
					"\n",
					"def get_fcast_dates(past_fcast_start_date, fcast_till_date):\n",
					"  \"\"\"\n",
					"  Get the cut-off dates for generating forecasts.\n",
					"  \"\"\"\n",
					"  past_fcast_from_date =  pd.to_datetime(past_fcast_start_date)\n",
					"#   future_fcast_start_date = (datetime.today() - MonthEnd(n=1)).strftime(\"%Y-%m-%d\")\n",
					"  future_fcast_start_date = (datetime.today()).strftime(\"%Y-%m-%d\")\n",
					"  fcast_till_date = pd.to_datetime(fcast_till_date)\n",
					"  num_days = fcast_till_date - (datetime.today() - MonthEnd(n=1))\n",
					"  return past_fcast_from_date, future_fcast_start_date, fcast_till_date, num_days\n",
					"\n",
					"def get_forecasted_exog(dof):\n",
					"  \"\"\"\n",
					"  Get the forecasted exogenous data to be used as input to forecast cash.\n",
					"  \"\"\"\n",
					"  past_fcasts = {}\n",
					"  final_model = {}\n",
					"  charge_spline_basis = {}\n",
					"  bdays_remaining_spline = {}\n",
					"  dep_lmbda = {}\n",
					"  past_fcasts[dof], final_model[dof], charge_spline_basis[dof], bdays_remaining_spline[dof], dep_lmbda[dof] = generate_past_fcasts(date=past_fcast_from_date.date(), exog=exog, dof=dof, till_date=future_fcast_start_date)\n",
					"  final_fcast_df = append_spline_basis(charge_spline_basis, bdays_remaining_spline, final_exog, dof)\n",
					"  forecast_exog_ = sm.add_constant(final_fcast_df)\n",
					"  return forecast_exog_, final_model, dep_lmbda, past_fcasts\n",
					"\n",
					"def get_future_fcasts(dof, forecast_exog_, final_model, dep_lmbda, past_fcasts):\n",
					"  \"\"\"\n",
					"  Forecast the cash amount and invert to original scale.\n",
					"  \"\"\"\n",
					"  forecast_summary = get_forecasts(forecast_exog_, final_model[dof], future_fcast_start_date, dep_lmbda[dof])\n",
					"  past_fcasts[dof]['DATE'] = past_fcasts[dof].index\n",
					"  forecast_summary['DATE'] = forecast_summary.index\n",
					"  columns_to_drop = ['mean_se', 'obs_ci_lower', 'obs_ci_upper']\n",
					"  forecast_summary_filtered = forecast_summary.drop(columns_to_drop, axis=1)\n",
					"  forecast_summary_filtered.columns = future_fcast_columns\n",
					"  forecast_summary_filtered = forecast_summary_filtered.assign(Payment_Net_Refunds=0)\n",
					"  final_fcast_df = pd.concat([past_fcasts[dof], forecast_summary_filtered])\n",
					"  return final_fcast_df"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "74c30997-0325-4c5d-8432-54e4ada825f6"
					}
				},
				"source": [
					"\"\"\"\n",
					"Load up the charge and cash data from CDW. Train and Forecast cash.\n",
					"\"\"\"\n",
					"df_c = spark.read.format(\"snowflake\").options(**options).option(\"query\", sql_stmt_charge).load()\n",
					"df_non_charge = spark.read.format(\"snowflake\").options(**options).option(\"query\", sql_stmt_cash).load()\n",
					"\n",
					"# Pre-Process charge data.\n",
					"df_c = pre_process_charge(df_c)\n",
					"\n",
					"# Pre-Process cash data.\n",
					"df_non_charge_pd = pre_process_cash(df_non_charge)\n",
					"\n",
					"# Prepare data for spline model training\n",
					"final_charge_df = prepare_charge_data(df_c, dummy_vars_charge_data, lags)\n",
					"final_df = append_dep_var(df_non_charge_pd, final_charge_df)\n",
					"endog, exog, _lambda = endog_exog_for_statsmodels(df=final_df, bx_cx=False)\n",
					"\n",
					"# Get the forecast dates, data and future forecast of cash.\n",
					"past_fcast_from_date, future_fcast_start_date, fcast_till_date, num_days = get_fcast_dates(past_fcast_start_, future_fcast_till_)\n",
					"final_exog = prep_data_final_fcast(exog, fcast_till_date, lags)\n",
					"forecast_exog_, final_model, dep_lmbda, past_fcasts = get_forecasted_exog(dof)\n",
					"final_fcast_df = get_future_fcasts(dof, forecast_exog_, final_model, dep_lmbda, past_fcasts)"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "222d2272-b40b-4e5d-9980-941627e79bc4"
					}
				},
				"source": [
					"\"\"\"\n",
					"Methods to generate Count Forecast.\n",
					"\"\"\" \n",
					"\n",
					"inverse_transform = lambda x: invert_bx_cx(x,lambda2)\n",
					"\n",
					"def train_prophet_model(train_df,holiday_df,additional_regressors=[]):\n",
					"  \"\"\"\n",
					"  Initialize the Prophet model, and add regressors.\n",
					"  \"\"\"\n",
					"  #invert_bx_cx 0.46324570681151295 0.8409843914766767\n",
					"  model = Prophet(weekly_seasonality=True,yearly_seasonality=True,holidays=holiday_df,interval_width=interval_width)\n",
					"  model.add_seasonality(name='monthly1', period=30.5, fourier_order=5)\n",
					"  for r in additional_regressors:\n",
					"    model.add_regressor(r)\n",
					"  model.fit(train_df)\n",
					"  return model\n",
					"\n",
					"def transform_and_get_lambda(df_non_charge_pd):\n",
					"  \"\"\"\n",
					"  Box-Cox transform cash and return the lambda param.\n",
					"  \"\"\"\n",
					"  df_non_charge_pd[response_var] = pd.to_numeric(df_non_charge_pd[response_var])\n",
					"  payment_arr = df_non_charge_pd[response_var]\n",
					"  payment_arr[payment_arr <= 0] = np.mean(payment_arr)\n",
					"  payment_arr, lambda1 = box_cox_transform(payment_arr)\n",
					"  return payment_arr, lambda1\n",
					"\n",
					"def add_count_data_and_transform(df_non_charge_pd, time_stamp_col):\n",
					"  \"\"\"\n",
					"  Add count column to the data for predicting count.\n",
					"  \"\"\"\n",
					"  payments_df = df_non_charge_pd.sort_index()\n",
					"  payments_df = payments_df[train_from_date:]\n",
					"  payments_df[count_col] = np.ones(len(payments_df))\n",
					"  payments_df[\"ds\"] = payments_df.index\n",
					"  print(payments_df.columns)\n",
					"  payments_df = payments_df.groupby([pd.Grouper(key=\"ds\", freq=\"D\")])[time_stamp_col, count_col].sum().reset_index()\n",
					"  payments_df.set_index(\"ds\", inplace=True)\n",
					"  payments_df[count_col], lambda2 = box_cox_transform(payments_df[count_col])\n",
					"  return payments_df, lambda2\n",
					"\n",
					"def add_hdays(year_list, country_name):\n",
					"  \"\"\"\n",
					"  Add holidays to the Prophet model based on year list and country name.\n",
					"  \"\"\"\n",
					"  holiday_df = make_holidays_df(year_list, country_name)\n",
					"  holiday_df[\"lower_window\"] = -2\n",
					"  holiday_df[\"upper_window\"] = 2\n",
					"  return holiday_df\n",
					"\n",
					"def transform_counts(payments_df, lambda2):\n",
					"  \"\"\"\n",
					"  Inverse Transform the count data and return the count.\n",
					"  \"\"\"\n",
					"  payments_df_count = pd.DataFrame({\"ds\":payments_df.index, \"y\":payments_df[count_col]})\n",
					"  allcounts = list(invert_bx_cx(payments_df_count.y, lambda2))\n",
					"  return allcounts\n",
					"\n",
					"def count_forecasts(count_model, payments_total1, lambda2, allcounts, num_days):\n",
					"  \"\"\"\n",
					"  Generate count forecast and invert to original scale.\n",
					"  \"\"\"\n",
					"  max_date = np.max(payments_total1.ds)\n",
					"  new_dates = [max_date + timedelta(days=i) for i in range(1, num_days+2)]\n",
					"  new_count_df = pd.DataFrame({\"ds\":new_dates,\"y\":[0 for i in range(1, num_days+2)]})\n",
					"  future2 = count_model.predict(new_count_df)\n",
					"  allcounts = allcounts + list(invert_bx_cx(future2[\"yhat\"], lambda2))\n",
					"  alldates = list(payments_total1.ds) + new_dates\n",
					"  count_fcast_df = pd.DataFrame({\"DATE\":alldates, count_col:allcounts})\n",
					"  return count_fcast_df\n",
					"\n",
					"def merge_fcasts(final_fcast_df, count_fcast_df):\n",
					"  \"\"\"\n",
					"  Merge count forecast with the cash forecast.\n",
					"  \"\"\"\n",
					"  final_fcast_df_with_count = final_fcast_df.merge(count_fcast_df)\n",
					"  final_fcast_df_with_count.index = final_fcast_df.index\n",
					"  final_fcast_df_with_count = final_fcast_df_with_count.fillna(method=\"bfill\")\n",
					"  final_fcast_df_with_count\n",
					"  return final_fcast_df_with_count"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "3bbda464-cd24-4ce5-a3a3-d4a5c98b5917"
					}
				},
				"source": [
					"\"\"\"\n",
					"Methods to plot the cash forecast.\n",
					"\"\"\"\n",
					"\n",
					"def get_fcasts(fcast_df):\n",
					"  fcast_df_weekly = fcast_df.resample('W-Mon').sum().reset_index()\n",
					"  fcast_df_monthly = fcast_df.resample('M').sum()\n",
					"  fcast_df_monthly.index = fcast_df_monthly.index - MonthBegin(n=1)\n",
					"  fcast_df_fortnight = fcast_df.resample('SM').sum().reset_index()\n",
					"  return {'Weekly_fcast':fcast_df_weekly, 'Fortnight_fcast':fcast_df_fortnight, 'Monthly_fcast':fcast_df_monthly, 'Daily':fcast_df}\n",
					"\n",
					"def plot_fcasts(df):\n",
					"  df = df[['Payment_Net_Refunds', 'Predicted_Net_Refund']]\n",
					"  df = df.loc['2018-01-01':'2021-07-30']\n",
					"  df_grouped = get_fcasts(df)\n",
					"  for name, df in df_grouped.items():\n",
					"    if 'Transaction Post Date' in list(df.columns):\n",
					"      df.index = df['Transaction Post Date']\n",
					"      df = df.drop(columns=['Transaction Post Date'])\n",
					"    df.plot(figsize=(12,8))\n",
					"  return None"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "15b53294-d7d5-4918-a92d-8f45e3b3f793"
					}
				},
				"source": [
					"\"\"\"\n",
					"Train the count model and get the forecast. Merge it with the cash forecast.\n",
					"\"\"\"\n",
					"# Transform the cash and count and return the lambda parameter of Bx-Cx Tr.\n",
					"payment_arr, lambda1 = transform_and_get_lambda(df_non_charge_pd)\n",
					"payments_df, lambda2 = add_count_data_and_transform(df_non_charge_pd, time_stamp_col)\n",
					"holiday_df = add_hdays(year_list, country_name)\n",
					"\n",
					"# Training data for count\n",
					"p_df_train = payments_df[train_from_date:future_fcast_start_date]\n",
					"p_df_train_ = pd.DataFrame({\"ds\":p_df_train.index, \"y\":p_df_train[count_col]})\n",
					"\n",
					"# Train count model. Do a Box-Cox transform of count.\n",
					"count_model = train_prophet_model(p_df_train_, holiday_df, additional_regressors=[])\n",
					"allcounts = transform_counts(p_df_train, lambda2)\n",
					"\n",
					"# Get Count Forecast.\n",
					"count_fcast_df = count_forecasts(count_model, p_df_train_, lambda2, allcounts, num_days.days)\n",
					"final_fcast_df_with_count = merge_fcasts(final_fcast_df, count_fcast_df)\n",
					"\n",
					"# # Plot the forecasts against the actuals to compare.\n",
					"# plot_fcasts(final_fcast_df_with_count)"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "37fd394e-bc2c-4fd4-8ad1-73686f3d302f"
					}
				},
				"source": [
					"plot_fcasts(final_fcast_df_with_count)"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "2b2fdea8-55c3-4995-8220-f5fb6db5c0f4"
					}
				},
				"source": [
					"\"\"\"\n",
					"Classes to generate the categorical predictions\n",
					"\"\"\" \n",
					"\n",
					"class RandomWalk():\n",
					"  def __init__(self, nx_G, is_directed, p, q):\n",
					"      self.G = nx_G\n",
					"      self.is_directed = is_directed\n",
					"      self.p = p\n",
					"      self.q = q\n",
					"\n",
					"  def node2vec_walk(self, walk_length, start_node):\n",
					"    '''\n",
					"    Simulate a random walk starting from start node.\n",
					"    '''\n",
					"    G = self.G\n",
					"    alias_nodes = self.alias_nodes\n",
					"    alias_edges = self.alias_edges\n",
					"    walk = [start_node]\n",
					"    while len(walk) < walk_length:\n",
					"        cur = walk[-1]\n",
					"        cur_nbrs = sorted(G.neighbors(cur))\n",
					"        if len(cur_nbrs) > 0:\n",
					"            if len(walk) == 1:\n",
					"                walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
					"            else:\n",
					"                prev = walk[-2]\n",
					"                next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
					"                    alias_edges[(prev, cur)][1])]\n",
					"                walk.append(next)\n",
					"        else:\n",
					"            break\n",
					"    return walk\n",
					"\n",
					"  def start_to_end_walk(self, start_node, end_node):\n",
					"    '''\n",
					"    Simulate a random walk starting from start node.\n",
					"    '''\n",
					"    G = self.G\n",
					"    alias_nodes = self.alias_nodes\n",
					"    alias_edges = self.alias_edges\n",
					"    walk = [start_node]\n",
					"    while walk[-1] != end_node:\n",
					"        cur = walk[-1]\n",
					"        cur_nbrs = sorted(G.neighbors(cur))\n",
					"        if len(cur_nbrs) > 0:\n",
					"            if len(walk) == 1:\n",
					"                walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
					"            else:\n",
					"                prev = walk[-2]\n",
					"                next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
					"                    alias_edges[(prev, cur)][1])]\n",
					"                walk.append(next)\n",
					"        else:\n",
					"            break\n",
					"    return walk\n",
					"\n",
					"  def simulate_walks(self, num_walks, walk_length):\n",
					"      '''\n",
					"      Repeatedly simulate random walks from each node.\n",
					"      '''\n",
					"      G = self.G\n",
					"      walks = []\n",
					"      nodes = list(G.nodes())\n",
					"      print ('Walk iteration:')\n",
					"      for walk_iter in range(num_walks):\n",
					"          print (str(walk_iter+1), '/', str(num_walks))\n",
					"          random.shuffle(nodes)\n",
					"          for node in nodes:\n",
					"              walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
					"      return walks\n",
					"\n",
					"  def get_alias_edge(self, src, dst):\n",
					"      '''\n",
					"      Get the alias edge setup lists for a given edge.\n",
					"      '''\n",
					"      G = self.G\n",
					"      p = self.p\n",
					"      q = self.q\n",
					"      unnormalized_probs = []\n",
					"      for dst_nbr in sorted(G.neighbors(dst)):\n",
					"          if dst_nbr == src:\n",
					"              unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
					"          elif G.has_edge(dst_nbr, src):\n",
					"              unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
					"          else:\n",
					"              unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
					"      norm_const = sum(unnormalized_probs)\n",
					"      normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
					"      return alias_setup(normalized_probs)\n",
					"\n",
					"  def preprocess_transition_probs(self):\n",
					"    '''\n",
					"    Preprocessing of transition probabilities for guiding the random walks.\n",
					"    '''\n",
					"    G = self.G\n",
					"    is_directed = self.is_directed\n",
					"    alias_nodes = {}\n",
					"    for node in G.nodes():\n",
					"        unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
					"        norm_const = sum(unnormalized_probs)\n",
					"        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
					"        alias_nodes[node] = alias_setup(normalized_probs)\n",
					"    alias_edges = {}\n",
					"    triads = {}\n",
					"\n",
					"    if is_directed:\n",
					"        for edge in G.edges():\n",
					"            alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
					"    else:\n",
					"        for edge in G.edges():\n",
					"            alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
					"            alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
					"    self.alias_nodes = alias_nodes\n",
					"    self.alias_edges = alias_edges\n",
					"    return\n",
					"\n",
					"def alias_setup(probs):\n",
					"\t'''\n",
					"\tCompute utility lists for non-uniform sampling from discrete distributions.\n",
					"\tRefer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
					"\tfor details\n",
					"\t'''\n",
					"\tK = len(probs)\n",
					"\tq = np.zeros(K)\n",
					"\tJ = np.zeros(K, dtype=np.int)\n",
					"\tsmaller = []\n",
					"\tlarger = []\n",
					"\tfor kk, prob in enumerate(probs):\n",
					"\t    q[kk] = K*prob\n",
					"\t    if q[kk] < 1.0:\n",
					"\t        smaller.append(kk)\n",
					"\t    else:\n",
					"\t        larger.append(kk)\n",
					"\twhile len(smaller) > 0 and len(larger) > 0:\n",
					"\t    small = smaller.pop()\n",
					"\t    large = larger.pop()\n",
					"\t    J[small] = large\n",
					"\t    q[large] = q[large] + q[small] - 1.0\n",
					"\t    if q[large] < 1.0:\n",
					"\t        smaller.append(large)\n",
					"\t    else:\n",
					"\t        larger.append(large)\n",
					"\treturn J, q\n",
					"\n",
					"def alias_draw(J, q):\n",
					"\t'''\n",
					"\tDraw sample from a non-uniform discrete distribution using alias sampling.\n",
					"\t'''\n",
					"\tK = len(J)\n",
					"\tkk = int(np.floor(np.random.rand()*K))\n",
					"\tif np.random.rand() < q[kk]:\n",
					"\t    return kk\n",
					"\telse:\n",
					"\t    return J[kk]\n",
					"      \n",
					"class ConstrainedGraphWalk():\n",
					"  \n",
					"  def __init__(self,df,graph_pickle_filename=None,is_directed=True):\n",
					"    self.df = df\n",
					"    self.is_directed = is_directed\n",
					"    if (graph_pickle_filename is not None):\n",
					"      print (\"reading preexisting graph\")\n",
					"      with open(graph_pickle_filename, \"rb\") as file:\n",
					"        self.G = pickle.load(file)\n",
					"    else:\n",
					"      self.G = self.create_graph(graph_model_path)\n",
					"      \n",
					"  def add_edge(self,G,c1,c2):\n",
					"    if G.has_edge(c1, c2):\n",
					"      G[c1][c2][\"weight\"] +=1\n",
					"    else:\n",
					"      G.add_edge(c1,c2,weight = 1)\n",
					"  \n",
					"  def create_graph(self,pickle_to=None):\n",
					"    print (\"create graph invoked\")\n",
					"    G = nx.DiGraph()\n",
					"    c = self.df.columns\n",
					"    count = 0\n",
					"    print (\"size of the dataframe \", len(df))\n",
					"    for iter,row in self.df.iterrows():\n",
					"      count +=1\n",
					"      if (count%100000 == 0): print(count)\n",
					"      c0 = \"start\"\n",
					"      c1 = str(c[0] + \"::\" + str(row[c[0]]))\n",
					"      self.add_edge(G,c0,c1)\n",
					"\n",
					"      for i in range(0,len(c)-1):\n",
					"        c1 = str(c[i] + \"::\" + str(row[c[i]]))\n",
					"        c2 = str(c[i+1] + \"::\" + str(row[c[i+1]]))\n",
					"        self.add_edge(G,c1,c2)\n",
					"    self.normalize_graph(G)\n",
					"    if (pickle_to is not None):\n",
					"      with open(pickle_to,\"wb\") as file:\n",
					"        pickle.dump(G,file)\n",
					"    return G\n",
					"\n",
					"  def normalize_graph(self,G):\n",
					"    print (\"normalizing graph to turn it into probablity distribution\")\n",
					"    for n in G.nodes():\n",
					"      weight_sum = np.sum([G[n][n1][\"weight\"] for n1 in G.neighbors(n)])\n",
					"      for n1 in G.neighbors(n):\n",
					"        G[n][n1][\"weight\"] = G[n][n1][\"weight\"]*1.0/weight_sum \n",
					"  \n",
					"  def preprocess_graph_for_walk(self,G=None):\n",
					"    if (G is None): G = self.G\n",
					"    self.rw = RandomWalk(G,True,2,2)\n",
					"    self.rw.preprocess_transition_probs()\n",
					"\n",
					"  def get_walks(self,numwalks=10):\n",
					"    walks = []\n",
					"    for i in range(0,numwalks):\n",
					"      walks.append(self.rw.node2vec_walk(6,\"start\"))\n",
					"    return walks\n",
					"    \n",
					"  def get_amounts(self,walks,discretizer,adjust_amount,p_totalval=-1,p_totalval_l=-1,p_totalval_u=-1):\n",
					"    walks_dict = defaultdict(lambda: [])\n",
					"    for awalk in walks:\n",
					"      awalk = awalk[1:]\n",
					"      for element in awalk:\n",
					"        col_val = element.split(\"::\")\n",
					"        if (col_val[0] == response_var):\n",
					"          walks_dict[col_val[0]].append(float(col_val[1]))\n",
					"        else:\n",
					"          walks_dict[col_val[0]].append(col_val[1])     \n",
					"    return_df = pd.DataFrame(walks_dict)\n",
					"    return_df[response_var] = (discretizer.inverse_transform(return_df[response_var].values.reshape(-1,1)).reshape(-1,)*max_val + min_val)\n",
					"    allvals = return_df[response_var].copy().values.reshape(-1,)\n",
					"    total = np.sum (return_df[response_var])\n",
					"    return_df[response_var] = return_df[response_var] * adjust_amount/total\n",
					"    if (p_totalval != -1):\n",
					"      return_df['Predicted_Net_Refunds'] = allvals * p_totalval/total\n",
					"    if (p_totalval_l != -1):\n",
					"      return_df['Predicted_Net_Refunds_Lower'] = allvals * p_totalval_l/total\n",
					"    if (p_totalval_u != -1):\n",
					"      return_df['Predicted_Net_Refunds_Upper'] = allvals * p_totalval_u/total\n",
					"    return return_df\n",
					"\n",
					"  \n",
					"  def get_amounts(self,walks,discretizer,adjust_amount,p_totalval=-1,p_totalval_l=-1,p_totalval_u=-1):\n",
					"    walks_dict = defaultdict(lambda: [])\n",
					"    for awalk in walks:\n",
					"      awalk = awalk[1:]\n",
					"      for element in awalk:\n",
					"        col_val = element.split(\"::\")\n",
					"        if (col_val[0] == 'Payment Net Refunds'):\n",
					"          walks_dict[col_val[0]].append(float(col_val[1]))\n",
					"        else:\n",
					"          walks_dict[col_val[0]].append(col_val[1])\n",
					"    return_df = pd.DataFrame(walks_dict)\n",
					"    return_df['Payment Net Refunds'] = (discretizer.inverse_transform(return_df['Payment Net Refunds'].values.reshape(-1,1)).reshape(-1,)*max_val + min_val)\n",
					"    allvals = return_df['Payment Net Refunds'].copy().values.reshape(-1,)\n",
					"    total = np.sum (return_df['Payment Net Refunds'])\n",
					"    return_df['Payment Net Refunds'] = return_df['Payment Net Refunds'] * adjust_amount/total\n",
					"    if (p_totalval != -1):\n",
					"      return_df['Predicted_Net_Refunds'] = allvals * p_totalval/total\n",
					"    if (p_totalval_l != -1):\n",
					"      return_df['Predicted_Net_Refunds_Lower'] = allvals * p_totalval_l/total\n",
					"    if (p_totalval_u != -1):\n",
					"      return_df['Predicted_Net_Refunds_Upper'] = allvals * p_totalval_u/total\n",
					"    return return_df\n",
					"  \n",
					"def fill_db_existing(predictions_df,df_full):\n",
					"  predictions_df[\"DATE\"] = predictions_df.index\n",
					"  max_date = np.max(predictions_df[predictions_df['Payment_Net_Refunds'] != 0].index)\n",
					"  max_date = max_date - timedelta(days=max_date.day) \n",
					"  prediction_existing_monthly = predictions_df[:max_date].copy()\n",
					"  prediction_existing_daily = predictions_df[max_date + timedelta(days=1):].copy()\n",
					"  prediction_existing_monthly = prediction_existing_monthly.groupby([pd.Grouper(key=\"DATE\", freq=\"M\")])[\"Payment_Net_Refunds\",\"Predicted_Net_Refund\",\"Predicted_Net_Refund_Lower\",\"Predicted_Net_Refund_Upper\"].sum().reset_index()\n",
					"  fractions_dict = defaultdict(lambda: [1.0,1.0,1.0])\n",
					"  fractions_dict1 = defaultdict(lambda: [1.0,1.0,1.0])\n",
					"  for iter,row in prediction_existing_monthly.iterrows():\n",
					"    fractions_dict[str(row[\"DATE\"].month)+\":\"+str(row[\"DATE\"].year)] = [row[\"Predicted_Net_Refund\"]/row[\"Payment_Net_Refunds\"],\n",
					"                                                                           row[\"Predicted_Net_Refund_Lower\"]/row[\"Payment_Net_Refunds\"], \n",
					"                                                                           row[\"Predicted_Net_Refund_Upper\"]/row[\"Payment_Net_Refunds\"]]\n",
					"  \n",
					"  for iter,row in prediction_existing_daily.iterrows():\n",
					"    if (row[\"Payment_Net_Refunds\"] != 0):\n",
					"      fractions_dict1[str(row[\"DATE\"].day)+str(row[\"DATE\"].month)+\":\"+str(row[\"DATE\"].year)] = [row[\"Predicted_Net_Refund\"]/row[\"Payment_Net_Refunds\"],\n",
					"                                                                                               row[\"Predicted_Net_Refund_Lower\"]/row[\"Payment_Net_Refunds\"],\n",
					"                                                                                               row[\"Predicted_Net_Refund_Upper\"]/row[\"Payment_Net_Refunds\"]]\n",
					"  df_full[\"DATE\"] = df_full.index\n",
					"  l = len (df_full)\n",
					"  v1 = list(np.ones(l))\n",
					"  v2 = list(np.ones(l))\n",
					"  v3 = list(np.ones(l))\n",
					"  i = 0\n",
					"  for iter,row in df_full.iterrows():\n",
					"    a = fractions_dict[str(row[\"DATE\"].month)+\":\"+str(row[\"DATE\"].year)]\n",
					"    b = fractions_dict1[str(row[\"DATE\"].day)+str(row[\"DATE\"].month)+\":\"+str(row[\"DATE\"].year)]\n",
					"    print (\"a,b\",a,b)        \n",
					"    v1[i] = row[\"Payment Net Refunds\"]*a[0]*b[0]\n",
					"    v2[i] = row[\"Payment Net Refunds\"]*a[1]*b[1]\n",
					"    v3[i] = row[\"Payment Net Refunds\"]*a[2]*b[2]\n",
					"    i = i + 1\n",
					"    if (i%100000 == 0): print (i)\n",
					"  df_full['Predicted_Net_Refund'] = v1\n",
					"  df_full['Predicted_Net_Refund_Lower'] = v2\n",
					"  df_full['Predicted_Net_Refund_Upper'] = v3\n",
					"  display(prediction_existing_monthly.head())\n",
					"  print (np.max(prediction_existing_monthly.DATE))\n",
					"  display(df_full.head())\n",
					"  return predictions_df, df_full\n",
					"  \n",
					"\n",
					"def generate_future_categories(df, predictions_df, df_full, graph_model_path, discretizer):\n",
					"  \"\"\"\n",
					"  Generate the future categories based on the cash forecast.\n",
					"  \"\"\"\n",
					"  # Fill full dataframe for future dates. \n",
					"  max_date = np.max(df_full.index)\n",
					"  dataframes_list = []\n",
					"  dates = predictions_df[max_date + timedelta(days=1):].index\n",
					"  cgw = ConstrainedGraphWalk(df, graph_model_path)\n",
					"  cgw.preprocess_graph_for_walk(cgw.G)\n",
					"  i = 0\n",
					"  for d in dates:\n",
					"    i+=1\n",
					"    print(f\"DATE : {d}\")\n",
					"    df_test = predictions_df[d:d]\n",
					"    totalval, num_count = np.sum(df_test['Payment_Net_Refunds']), int(np.sum(df_test['Counts']))\n",
					"    p_totalval =  np.sum(df_test['Predicted_Net_Refund'])\n",
					"#     if (totalval == 0):\n",
					"#       totalval = p_totalval\n",
					"    p_totalval_l = np.sum(df_test['Predicted_Net_Refund_Lower'])\n",
					"    p_totalval_u = np.sum(df_test['Predicted_Net_Refund_Upper'])\n",
					"    print (f\"date {i} : {d} : {totalval},{num_count}\")\n",
					"    num_count = num_count%50000\n",
					"    walks = cgw.get_walks(num_count)\n",
					"    return_df = cgw.get_amounts(walks,discretizer,totalval,p_totalval,p_totalval_l,p_totalval_u)\n",
					"    return_df[\"DATE\"] = [d for i in range(0,len(return_df))]\n",
					"    dataframes_list.append(return_df)\n",
					"    #break\n",
					"  return dataframes_list\n",
					"\n",
					"\n",
					"\n",
					"def create_forecast_dataframe(df_full, dataframes_list):\n",
					"  \"\"\"\n",
					"  Create the final forecast dataframe from the list of forecast dataframes.\n",
					"  \"\"\"\n",
					"  df_full = df_full.rename(columns=fcast_col_name_dict)\n",
					"  dataframes_list = [ df_full ] + dataframes_list\n",
					"  final_df = pd.concat(dataframes_list,ignore_index=True)\n",
					"  final_df = final_df.rename(columns=fcast_col_name_dict_1)\n",
					"  return final_df\n",
					"\n",
					"def pre_process_and_discretize(df):\n",
					"  \"\"\"\n",
					"  Preocess the data for the Graph model.\n",
					"  \"\"\"\n",
					"  df_ = df[train_graph_from_date:] # we can ignore older data. We want to find distribution as it goes along. \n",
					"  x = df_[response_var].values.reshape(-1,1)\n",
					"  min_val = np.min(df_[response_var])\n",
					"  max_val = np.max(df_[response_var])\n",
					"  df_[response_var] = (df_[response_var] - min_val)/max_val\n",
					"  discretizer = KBinsDiscretizer(n_bins=5000, encode='ordinal', strategy='uniform')\n",
					"  df_[response_var] = discretizer.fit_transform(df_[response_var].values.reshape(-1,1)).reshape(-1,) \n",
					"  return df_, max_val, min_val, discretizer\n",
					"\n",
					"\n",
					"def push_forecasts_to_cdw(df):\n",
					"  \"\"\"\n",
					"  Push the forecasted data to CDW.\n",
					"  \"\"\"\n",
					"  fcast_df_ = spark.createDataFrame(df)\n",
					"  options = dict(sfUrl=sfUrl,\n",
					"                 sfUser=user,\n",
					"                 sfPassword=password,\n",
					"                 sfDatabase=sfDatabase,\n",
					"                 sfSchema=sfSchema,\n",
					"                 sfWarehouse=sfWarehouse)\n",
					"  fcast_df_.write \\\n",
					"    .format(\"snowflake\") \\\n",
					"    .options(**options) \\\n",
					"    .option(\"dbtable\", table_name) \\\n",
					"    .save()\n",
					"  \n",
					"def update_future_facst(table_, past_fcast_till_date, future_df):\n",
					"  df_preds = spark.table(f\"RCI_FIRST_PASS_DATA.{table_}\").toPandas()\n",
					"  df_preds_past = df_preds.copy(deep=True)\n",
					"  df_preds_past = df_preds_past[df_preds_past['DATE'] < past_fcast_till_date]\n",
					"  future_df = future_df[future_df['DATE'] >= past_fcast_till_date]\n",
					"  fcast_df_ = pd.concat([df_preds_past, future_df])\n",
					"  return fcast_df_"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "556d477d-83bc-4d57-9060-99c8813c2508"
					}
				},
				"source": [
					"\"\"\"\n",
					"Train the graph model and get Categories predictions.\n",
					"\"\"\"\n",
					"df_non_charge_pd = pre_process_cash(df_non_charge)\n",
					"df = df_non_charge_pd.copy(deep=True)\n",
					"df.index = df[time_stamp_col]\n",
					"df = df.drop(columns=[time_stamp_col, discharge_date]).sort_index()\n",
					"df = df[df[response_var]>0]\n",
					"df = df.astype({'Payment Net Refunds': 'float'})\n",
					"print(np.sum(df.loc['2018-01-01':'2018-12-31']['Payment Net Refunds']))\n",
					"df_full = df.copy(deep=True)\n",
					"df_full = df_full[past_fcast_start_:]\n",
					"df_full = df_full.astype({'Payment Net Refunds': 'float'})\n",
					"print(np.sum(df_full.loc['2018-01-01':'2018-12-31']['Payment Net Refunds']))\n",
					"\n",
					"\n",
					"df_non_charge_, max_val, min_val, discretizer = pre_process_and_discretize(df)\n",
					"\n",
					"# Based on overall predictions perform the category predictions.\n",
					"predictions_df = final_fcast_df_with_count.set_index(\"DATE\")\n",
					"predictions_df[count_col].replace(to_replace=0, method='bfill', inplace=True)\n",
					"predictions_df, df_full_ = fill_db_existing(predictions_df, df_full)\n",
					"\n",
					"# Generate future categories\n",
					"df_non_charge_, max_val, min_val, discretizer = pre_process_and_discretize(df)\n",
					"dataframes_list = generate_future_categories(df_non_charge_, predictions_df, df_full_, graph_model_path, discretizer)\n",
					"final_df = create_forecast_dataframe(df_full_, dataframes_list)\n",
					"final_df1 = final_df[final_df[\"DATE\"] >= past_fcast_start_]\n",
					"# push_forecasts_to_cdw(final_df1)\n",
					"\n",
					"# Only update the future forecasts.\n",
					"table_ = \"cash_flow_categories_spline\"\n",
					"past_fcast_till_date = (datetime.today() - MonthEnd(n=1)).strftime(\"%Y-%m-%d\")\n",
					"modified_fcast_df = update_future_facst(table_, past_fcast_till_date, final_df1)\n",
					"\n",
					"# Push the cash forecast to DBFS\n",
					"final_fcast_df_spark_ = spark.createDataFrame(modified_fcast_df)\n",
					"final_fcast_df_spark_.write.mode(\"overwrite\").format(\"delta\").saveAsTable(f\"RCI_FIRST_PASS_DATA.{table_}\")"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "2e9c9b89-0005-41b8-857d-a03f3ebfaaf5"
					}
				},
				"source": [
					"modified_fcast_df"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "bf8c98e2-41c7-4163-b7e4-24a8f0ea02a9"
					}
				},
				"source": [
					"%sql\n",
					"DROP TABLE rci_first_pass_data.cash_flow_categories_spline_Agg;\n",
					"\n",
					"CREATE TABLE rci_first_pass_data.cash_flow_categories_spline_Agg\n",
					"USING DELTA\n",
					"AS (SELECT PATIENT_TYPE_NAME AS Patient_Type\n",
					",REGION_NAME AS Region\n",
					",Transaction_Financial_Class_Name AS Transaction_Financial_Class\n",
					",Transaction_Major_Payor AS Transaction_Major_Payor\n",
					",CAST(DATE AS DATE) AS Date\n",
					",year(DATE) AS Calendar_Year\n",
					",month(DATE) AS Calendar_Month\n",
					",weekofyear(DATE) AS Calendar_Week\n",
					",SUM(Payment_Net_Refunds) AS Actual_Payment\n",
					",SUM(Predicted_Net_Refunds) AS Predicted_Payment\n",
					",SUM(Predicted_Net_Refunds_Lower) AS Predicted_Payment_Lower\n",
					",SUM(Predicted_Net_Refunds_Upper) AS Predicted_Payment_Upper\n",
					"FROM rci_first_pass_data.cash_flow_categories_spline\n",
					"GROUP BY PATIENT_TYPE_NAME\n",
					",REGION_NAME\n",
					",Transaction_Financial_Class_Name\n",
					",Transaction_Major_Payor\n",
					",DATE)"
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "52c5404e-fbb9-4bdc-a408-5ae930abec4d"
					}
				},
				"source": [
					"%sql SELECT * FROM  RCI_FIRST_PASS_DATA.cash_flow_categories_spline WHERE DATE > \"2021-08-08\" and DATE < \"2021-08-10\""
				],
				"attachments": null,
				"execution_count": 0
			},
			{
				"cell_type": "code",
				"metadata": {
					"application/vnd.databricks.v1+cell": {
						"title": "",
						"showTitle": false,
						"nuid": "0739b766-a5fe-45d9-b9d4-34cdb723908a"
					}
				},
				"source": [
					"%sql SELECT * FROM  RCI_FIRST_PASS_DATA.cash_flow_categories_spline WHERE Predicted_Net_Refunds > 100 and Predicted_Net_Refunds < 200"
				],
				"attachments": null,
				"execution_count": 0
			}
		]
	}
}